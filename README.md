# Stock Analysis Bot

A comprehensive Python application for stock analysis that combines technical analysis, analyst ratings, news sentiment, and AI-powered insights with robust data handling and production-ready features.

## ğŸš€ Recent Updates

### âœ… **Production-Ready Features**
- **Comprehensive NaN Handling**: Robust cleaning of all NaN-like values before database insertion
- **Safe MySQL Insertion**: Protected database operations with comprehensive error handling
- **Clean Codebase**: Removed debug messages and organized functions
- **Modular Design**: Separate functions for different analysis stages
- **Enhanced Logging**: Professional logging with success/failure indicators

### ğŸ”§ **Technical Improvements**
- **Multi-layered NaN Cleaning**: Handles `np.nan`, `float('nan')`, string representations, and more
- **Cell-by-cell Inspection**: Final verification to ensure no problematic values remain
- **Backward Compatibility**: Maintains existing functionality while adding safety layers
- **Error Recovery**: Graceful handling of database and API failures

## Project Structure

```
PythonProject1/
â”œâ”€â”€ ai/                     # AI analysis modules
â”‚   â”œâ”€â”€ analysis_runner.py  # Main AI analysis orchestration
â”‚   â””â”€â”€ connector.py        # OpenAI API integration
â”œâ”€â”€ config/                 # Configuration management
â”‚   â”œâ”€â”€ config.json         # Configuration file
â”‚   â””â”€â”€ config.py           # Configuration loader
â”œâ”€â”€ data_processing/        # Data processing modules
â”‚   â”œâ”€â”€ analysts.py         # Analyst rating processing
â”‚   â”œâ”€â”€ news.py            # News sentiment processing
â”‚   â””â”€â”€ technical.py       # Technical analysis processing
â”œâ”€â”€ database/              # Database management
â”‚   â”œâ”€â”€ connection.py      # Database connection and operations
â”‚   â”œâ”€â”€ models.py          # Data models and utilities
â”‚   â””â”€â”€ schema.py          # Database schema management
â”œâ”€â”€ tests/                 # Test modules
â”‚   â””â”€â”€ inner_test.py      # Internal testing utilities
â”œâ”€â”€ utils/                 # Utility functions
â”‚   â””â”€â”€ helpers.py         # Helper utilities (NaN cleaning, safe insertion)
â”œâ”€â”€ main.py                # Main application entry point
â”œâ”€â”€ test_comprehensive_nan_cleaning.py  # Comprehensive NaN cleaning tests
â”œâ”€â”€ test_nan_fix.py        # NaN handling tests
â””â”€â”€ requirements.txt       # Python dependencies
```

## Features

### ğŸ¯ **Two-Stage Analysis Approach**

The application uses a sophisticated two-stage analysis approach with enhanced safety features:

#### Stage 1: Individual AI Analyses
- **Purpose**: Analyze each ticker by each data source (technical, analysts, news)
- **Function**: `run_ai_analysis_for_tickers(tickers, window)`
- **Output**: DataFrame with individual analysis results for each ticker and data source
- **Database**: Results stored in `ai_analysis` table with `analysis_type` field
- **Safety**: Comprehensive NaN cleaning before database insertion

#### Stage 2: Consolidated Analysis
- **Purpose**: Analyze tickers using all analysis types together with weights
- **Function**: `run_consolidated_ai_analysis(tickers, window, model)`
- **Input**: Fetches data from `ai_analysis` table (Stage 1 results)
- **Output**: DataFrame with consolidated weighted analysis for each ticker
- **Database**: Results stored in `ai_analysis` table with `analysis_type = 'all'`
- **Safety**: Protected insertion with multi-layered NaN handling

### ğŸ“Š **Data Sources**

1. **Technical Analysis**: Stock price data with technical indicators
2. **Analyst Ratings**: Professional analyst recommendations and ratings
3. **News Sentiment**: Company news sentiment analysis

### ğŸ¤– **AI Integration**

- OpenAI GPT models for intelligent analysis
- Configurable system messages for different analysis types
- Weighted consolidation of multiple analysis sources
- Token usage tracking and cost management
- Robust JSON response parsing with fallback handling

## Installation

1. Clone the repository
2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
3. Configure your settings in `config/config.json`

## Configuration

Edit `config/config.json` to set up:

- **API Keys**: OpenAI, Alpha Vantage
- **Database**: MySQL connection details
- **Tickers**: List of stock symbols to analyze
- **Analysis Settings**: Models, system messages, weights
- **Time Windows**: Analysis periods

## Usage

### ğŸƒâ€â™‚ï¸ **Main Application**

Run the complete two-stage analysis with enhanced safety:
```bash
python main.py
```

### ğŸ”§ **Individual Functions**

#### Run Only Individual Analyses (Stage 1)
```python
from main import run_individual_analyses_only
run_individual_analyses_only()
```

#### Run Only Consolidated Analysis (Stage 2)
```python
from main import run_consolidated_analyses_only
run_consolidated_analyses_only()
```

#### Run Only Raw Data Insertion
```python
from main import run_raw_data_only
run_raw_data_only()
```

### ğŸ§ª **Testing**

Test the comprehensive NaN cleaning:
```bash
python test_comprehensive_nan_cleaning.py
```

Test the NaN fix functionality:
```bash
python test_nan_fix.py
```

## Database Schema

### Main Tables
- `daily_stock`: Technical stock data
- `analyst_rating`: Analyst recommendations
- `news_rating`: News sentiment data
- `ai_analysis`: AI analysis results (both individual and consolidated)

### AI Analysis Table Structure
```sql
CREATE TABLE ai_analysis (
    event_date DATETIME,
    insertion_time DATETIME,
    company_ticker VARCHAR(10),
    analysis_type VARCHAR(50),  -- 'technical_analysis', 'analysts_rating', 'news_analysis', 'all'
    wights JSON,                -- Weights used for consolidated analysis
    grade FLOAT,                -- AI analysis score (0-1)
    text_analysis TEXT,         -- AI analysis explanation
    AI_model VARCHAR(50),       -- GPT model used
    prompt_tokens INT,          -- Input tokens used
    excution_tokens INT         -- Output tokens used
);
```

## Analysis Workflow

1. **Data Collection**: Fetch raw data from APIs
2. **Stage 1 - Individual Analysis**: 
   - Analyze each ticker by each data source
   - Clean all NaN values using comprehensive cleaning
   - Store results in `ai_analysis` table using safe insertion
3. **Stage 2 - Consolidated Analysis**:
   - Fetch individual analysis results
   - Apply weighted consolidation
   - Clean all NaN values using comprehensive cleaning
   - Store final results in `ai_analysis` table using safe insertion

## Key Functions

### AI Analysis Runner (`ai/analysis_runner.py`)

- `run_ai_analysis_for_tickers()`: Stage 1 - Individual analyses
- `run_consolidated_ai_analysis()`: Stage 2 - Consolidated analysis
- `insert_raw_data_only()`: Insert raw data without AI analysis
- `_run_openai_analysis()`: Enhanced OpenAI integration with robust parsing

### Database Connection (`database/connection.py`)

- `insert_data()`: Insert DataFrames into database tables with NaN handling
- `get_data()`: Fetch data for specific tickers and tables
- `stock_data_brain()`: Process and insert stock data
- `analysts_data_brain()`: Process and insert analyst data
- `company_news_data_brain()`: Process and insert news data

### Utility Functions (`utils/helpers.py`)

- `clean_dataframe_for_mysql()`: Comprehensive NaN cleaning for all data types
- `safe_mysql_insert()`: Protected database insertion with error handling

## ğŸ”’ **Enhanced Error Handling**

The application includes comprehensive error handling with production-ready features:

### NaN Value Handling
- **Multi-layered cleaning**: Handles all types of NaN values
- **String representations**: Cleans 'nan', 'NaN', 'NAN', 'null', 'NULL', etc.
- **Type safety**: Handles numpy NaN, Python NaN, and string representations
- **Cell-by-cell verification**: Final check to ensure no problematic values remain

### Database Safety
- **Protected insertion**: Safe MySQL insertion with comprehensive cleaning
- **Error recovery**: Graceful handling of database connection failures
- **Transaction management**: Proper rollback on errors
- **Duplicate handling**: INSERT IGNORE for duplicate prevention

### API Integration
- **Rate limiting**: Handles API rate limits gracefully
- **Response parsing**: Robust JSON parsing with fallback handling
- **Token tracking**: Monitors OpenAI token usage
- **Error logging**: Detailed error reporting for debugging

## ğŸ“ **Enhanced Logging**

All operations are logged with professional formatting:
- **INFO**: Normal operations with clear success indicators
- **WARNING**: Non-critical issues with actionable information
- **ERROR**: Critical failures with detailed error context
- **Success Indicators**: âœ…/âŒ symbols for quick status identification

## ğŸ§ª **Testing & Validation**

### Comprehensive Testing
- **NaN Cleaning Tests**: Verify all NaN types are properly handled
- **Database Insertion Tests**: Validate safe insertion functionality
- **Integration Tests**: End-to-end workflow validation
- **Error Handling Tests**: Verify graceful failure handling

### Test Scripts
- `test_comprehensive_nan_cleaning.py`: Comprehensive NaN handling validation
- `test_nan_fix.py`: Specific NaN fix functionality testing
- `test_two_stage_analysis.py`: Two-stage analysis workflow testing

## ğŸš€ **Production Features**

### Code Quality
- **Clean Architecture**: Modular design with clear separation of concerns
- **Type Safety**: Proper type hints and validation
- **Documentation**: Comprehensive docstrings and comments
- **Error Recovery**: Graceful handling of all failure scenarios

### Performance
- **Efficient Data Processing**: Optimized DataFrame operations
- **Memory Management**: Proper cleanup and resource management
- **Batch Operations**: Efficient database batch insertions
- **Caching**: Intelligent data caching where appropriate

### Monitoring
- **Token Usage Tracking**: Monitor OpenAI API costs
- **Performance Metrics**: Track analysis completion times
- **Error Monitoring**: Comprehensive error tracking and reporting
- **Success Rates**: Monitor analysis success rates

## Contributing

1. Follow the existing code structure and patterns
2. Add appropriate logging with success/failure indicators
3. Include comprehensive error handling
4. Test NaN handling for any new data processing
5. Update documentation for new features
6. Run comprehensive tests before submitting

## License

This project is for educational and research purposes.

---

## ğŸ¯ **Quick Start**

1. **Setup**: Install dependencies and configure API keys
2. **Test**: Run `python test_comprehensive_nan_cleaning.py` to verify setup
3. **Run**: Execute `python main.py` for complete analysis
4. **Monitor**: Check logs for success indicators and any issues

The application is now **production-ready** with robust error handling, comprehensive data cleaning, and professional logging! ğŸš€ 

## ğŸ—ï¸ **Code Architecture & Function Relationships**

### **Main Application Flow**

```
main.py
â”œâ”€â”€ main()                          # ğŸ¯ Entry point
â”‚   â”œâ”€â”€ ai_runner.insert_raw_data_only()     # ğŸ“Š Stage 1: Raw data insertion
â”‚   â”œâ”€â”€ ai_runner.run_ai_analysis_for_tickers()  # ğŸ¤– Stage 2: Individual AI analyses
â”‚   â”‚   â”œâ”€â”€ db_connection.get_data()         # ğŸ“¥ Fetch data from DB
â”‚   â”‚   â”œâ”€â”€ _run_openai_analysis()           # ğŸ§  Run OpenAI analysis
â”‚   â”‚   â””â”€â”€ safe_mysql_insert()              # ğŸ”’ Safe DB insertion
â”‚   â””â”€â”€ ai_runner.run_consolidated_ai_analysis()  # ğŸ¯ Stage 3: Consolidated analysis
â”‚       â”œâ”€â”€ _fetch_ai_analysis_data()        # ğŸ“¥ Fetch AI analysis data
â”‚       â”œâ”€â”€ _run_openai_analysis()           # ğŸ§  Run consolidated OpenAI analysis
â”‚       â””â”€â”€ safe_mysql_insert()              # ğŸ”’ Safe DB insertion
```

### **Module Dependencies & Data Flow**

```
ğŸ“ ai/analysis_runner.py
â”œâ”€â”€ run_ai_analysis_for_tickers()
â”‚   â”œâ”€â”€ db_connection.get_data() â†’ ğŸ“ database/connection.py
â”‚   â”œâ”€â”€ _run_openai_analysis() â†’ ğŸ¤– OpenAI API
â”‚   â””â”€â”€ safe_mysql_insert() â†’ ğŸ“ utils/helpers.py
â”‚
â”œâ”€â”€ run_consolidated_ai_analysis()
â”‚   â”œâ”€â”€ _fetch_ai_analysis_data() â†’ ğŸ“ database/connection.py
â”‚   â”œâ”€â”€ _create_consolidated_system_message() â†’ ğŸ“ config/config.py
â”‚   â”œâ”€â”€ _run_openai_analysis() â†’ ğŸ¤– OpenAI API
â”‚   â””â”€â”€ safe_mysql_insert() â†’ ğŸ“ utils/helpers.py
â”‚
â””â”€â”€ insert_raw_data_only()
    â”œâ”€â”€ stock_data_brain() â†’ ğŸ“ data_processing/technical.py
    â”œâ”€â”€ analysts_data_brain() â†’ ğŸ“ data_processing/analysts.py
    â””â”€â”€ company_news_data_brain() â†’ ğŸ“ data_processing/news.py
```

### **Database Operations Flow**

```
ğŸ“ database/connection.py
â”œâ”€â”€ insert_data()
â”‚   â”œâ”€â”€ clean_dataframe_for_mysql() â†’ ğŸ“ utils/helpers.py
â”‚   â”œâ”€â”€ MySQL INSERT IGNORE
â”‚   â””â”€â”€ Error handling & rollback
â”‚
â”œâ”€â”€ get_data()
â”‚   â”œâ”€â”€ MySQL SELECT query
â”‚   â””â”€â”€ DataFrame creation
â”‚
â”œâ”€â”€ stock_data_brain()
â”‚   â”œâ”€â”€ stocks_technical_analysis.get_stock_data() â†’ ğŸ“ data_processing/technical.py
â”‚   â””â”€â”€ insert_data()
â”‚
â”œâ”€â”€ analysts_data_brain()
â”‚   â”œâ”€â”€ analysts_rating.avg_analyst_rating() â†’ ğŸ“ data_processing/analysts.py
â”‚   â””â”€â”€ insert_data()
â”‚
â””â”€â”€ company_news_data_brain()
    â”œâ”€â”€ company_news.av_get_json() â†’ ğŸ“ data_processing/news.py
    â”œâ”€â”€ company_news.news_to_df_single()
    â”œâ”€â”€ company_news.merge_dfs_no_dupes()
    â””â”€â”€ insert_data()
```

### **Data Processing Pipeline**

```
ğŸ“ data_processing/
â”œâ”€â”€ technical.py
â”‚   â”œâ”€â”€ get_stock_data() â†’ ğŸ“Š Alpha Vantage API
â”‚   â”œâ”€â”€ Technical indicators calculation
â”‚   â””â”€â”€ DataFrame formatting
â”‚
â”œâ”€â”€ analysts.py
â”‚   â”œâ”€â”€ avg_analyst_rating() â†’ ğŸ“Š Alpha Vantage API
â”‚   â”œâ”€â”€ Analyst rating aggregation
â”‚   â””â”€â”€ DataFrame formatting
â”‚
â””â”€â”€ news.py
    â”œâ”€â”€ av_get_json() â†’ ğŸ“Š Alpha Vantage API
    â”œâ”€â”€ news_to_df_single() â†’ ğŸ“° News processing
    â”œâ”€â”€ merge_dfs_no_dupes() â†’ ğŸ”„ Data deduplication
    â””â”€â”€ DataFrame formatting
```

### **AI Analysis Pipeline**

```
ğŸ¤– OpenAI Integration
â”œâ”€â”€ _run_openai_analysis()
â”‚   â”œâ”€â”€ OpenAI API call
â”‚   â”œâ”€â”€ JSON response parsing
â”‚   â”œâ”€â”€ parse_text_analysis_column() â†’ ğŸ” Response parsing
â”‚   â””â”€â”€ Score & explanation extraction
â”‚
â”œâ”€â”€ Individual Analysis
â”‚   â”œâ”€â”€ Technical analysis â†’ System message + Stock data
â”‚   â”œâ”€â”€ Analyst analysis â†’ System message + Analyst data
â”‚   â””â”€â”€ News analysis â†’ System message + News data
â”‚
â””â”€â”€ Consolidated Analysis
    â”œâ”€â”€ Fetch individual results from DB
    â”œâ”€â”€ Weighted consolidation
    â””â”€â”€ Final AI analysis with weights
```

### **Safety & Error Handling Flow**

```
ğŸ”’ Safety Layer
â”œâ”€â”€ safe_mysql_insert()
â”‚   â”œâ”€â”€ clean_dataframe_for_mysql() â†’ ğŸ§¹ NaN cleaning
â”‚   â”œâ”€â”€ Database insertion attempt
â”‚   â”œâ”€â”€ Error handling
â”‚   â””â”€â”€ Success/failure reporting
â”‚
â”œâ”€â”€ clean_dataframe_for_mysql()
â”‚   â”œâ”€â”€ Multi-layered NaN replacement
â”‚   â”œâ”€â”€ Cell-by-cell verification
â”‚   â”œâ”€â”€ Type safety checks
â”‚   â””â”€â”€ Comprehensive logging
â”‚
â””â”€â”€ Error Recovery
    â”œâ”€â”€ Database connection failures â†’ Retry logic
    â”œâ”€â”€ API rate limiting â†’ Graceful handling
    â”œâ”€â”€ Data validation â†’ Fallback strategies
    â””â”€â”€ AI analysis failures â†’ Error logging
```

### **Configuration & Utilities**

```
ğŸ“ config/
â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ get() â†’ ğŸ“„ JSON config loading
â”‚   â”œâ”€â”€ Database credentials
â”‚   â”œâ”€â”€ API keys
â”‚   â””â”€â”€ Analysis parameters
â”‚
ğŸ“ utils/
â””â”€â”€ helpers.py
    â”œâ”€â”€ clean_dataframe_for_mysql() â†’ ğŸ§¹ Data cleaning
    â””â”€â”€ safe_mysql_insert() â†’ ğŸ”’ Safe insertion
```

### **Function Call Hierarchy**

```
main() [main.py]
â”œâ”€â”€ insert_raw_data_only() [ai/analysis_runner.py]
â”‚   â”œâ”€â”€ stock_data_brain() [database/connection.py]
â”‚   â”‚   â””â”€â”€ get_stock_data() [data_processing/technical.py]
â”‚   â”œâ”€â”€ analysts_data_brain() [database/connection.py]
â”‚   â”‚   â””â”€â”€ avg_analyst_rating() [data_processing/analysts.py]
â”‚   â””â”€â”€ company_news_data_brain() [database/connection.py]
â”‚       â””â”€â”€ av_get_json() [data_processing/news.py]
â”‚
â”œâ”€â”€ run_ai_analysis_for_tickers() [ai/analysis_runner.py]
â”‚   â”œâ”€â”€ get_data() [database/connection.py]
â”‚   â”œâ”€â”€ _run_openai_analysis() [ai/analysis_runner.py]
â”‚   â””â”€â”€ safe_mysql_insert() [utils/helpers.py]
â”‚       â””â”€â”€ clean_dataframe_for_mysql() [utils/helpers.py]
â”‚
â””â”€â”€ run_consolidated_ai_analysis() [ai/analysis_runner.py]
    â”œâ”€â”€ _fetch_ai_analysis_data() [ai/analysis_runner.py]
    â”œâ”€â”€ _run_openai_analysis() [ai/analysis_runner.py]
    â””â”€â”€ safe_mysql_insert() [utils/helpers.py]
        â””â”€â”€ clean_dataframe_for_mysql() [utils/helpers.py]
```

### **Data Transformation Flow**

```
Raw Data Sources
â”œâ”€â”€ Alpha Vantage API â†’ Technical Data
â”œâ”€â”€ Alpha Vantage API â†’ Analyst Data
â””â”€â”€ Alpha Vantage API â†’ News Data
    â†“
Data Processing Modules
â”œâ”€â”€ technical.py â†’ Technical indicators
â”œâ”€â”€ analysts.py â†’ Rating aggregation
â””â”€â”€ news.py â†’ Sentiment analysis
    â†“
Database Storage
â”œâ”€â”€ daily_stock table
â”œâ”€â”€ analyst_rating table
â””â”€â”€ news_rating table
    â†“
AI Analysis
â”œâ”€â”€ Individual analysis â†’ ai_analysis table
â””â”€â”€ Consolidated analysis â†’ ai_analysis table
    â†“
Final Results
â””â”€â”€ Weighted stock recommendations
```

This architecture ensures:
- **ğŸ”’ Safety**: Multi-layered error handling and data validation
- **ğŸ“Š Efficiency**: Optimized data flow and processing
- **ğŸ§ª Testability**: Modular design for easy testing
- **ğŸ“ˆ Scalability**: Clear separation of concerns
- **ğŸ›¡ï¸ Reliability**: Comprehensive error recovery mechanisms 